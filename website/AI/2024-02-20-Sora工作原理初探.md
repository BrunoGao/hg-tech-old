---
title: Sora工作原理初探
authors: [only]
tags: [Sora]
date: 2024-02-20
summary: >-
  Sora是一个引起广泛关注的AI项目，其核心作者是Bill Peebles和Tim Brooks。该项目利用Encoder和Decoder从视频中提取和生成视频latent，同时使用Diffusion Transformer模块进行去噪处理。Sora重新训练了视频的Encoder以保留视频的时空信息，并通过scaling law扩大模型规模。该项目支持不同长度和分辨率的视频输入，采用时空联合建模方法进行视频编码。此外，Sora还涉及到Diffusion Transformer和Transformer Decoder等技术，为实现视频生成和模拟物理世界迈出重要一步。<br>
  Sora is an AI project that has attracted widespread attention. Its core authors are Bill Peebles and Tim Brooks. The project utilizes Encoder and Decoder to extract and generate video latents, while using the Diffusion Transformer module for denoising. Sora retrained a video Encoder to preserve the spatiotemporal information of videos and scaled the model using the scaling law. The project supports inputs of different lengths and resolutions, employing a spatial-temporal joint modeling approach for video encoding. Additionally, Sora involves technologies like Diffusion Transformer and Transformer Decoder, taking a significant step towards video generation and simulating the physical world.
---

>[https://openai.com/sora](https://link.zhihu.com/?target=https%3A//openai.com/sora) 工作一出，引起社会各界广泛关注。中美AI的差距进一步扩大，中美人才培养体系的差距等等言论，甚嚣尘上。其实在Sora工作出来之前，文生视频领域，华人学者和产业界的参与度还是非常高的，包括魔搭社区的VGen([https://modelscope.cn/models/iic/i2vgen-xl/summary](https://link.zhihu.com/?target=https%3A//modelscope.cn/models/iic/i2vgen-xl/summary))系列， HeyGen在数字人场景也得到了大家的认可，清华的朱军老师团队在diffusion transformmers结合场景也深耕多年有很多的产出，例如U-Vit([https://github.com/baofff/U-ViT](https://link.zhihu.com/?target=https%3A//github.com/baofff/U-ViT))工作。 那么Sora到底是谁做的，怎么做的，本篇文章将从sora的technical report进行详细分析，给出大致的技术猜测。 同时我们也相信国内同行在有着深厚积累的情况下，也能百家争鸣，紧追不舍。

## **作者是谁**



![img](https://heguang-tech-1300607181.cos.ap-shanghai.myqcloud.com/uPic/v2-2346ae4b81bc9e854194e7eb302b9521_1440w.webp)

Sora团队负责人的Twitter

Sora的核心作者是Bill Peebles([https://www.wpeebles.com/](https://link.zhihu.com/?target=https%3A//www.wpeebles.com/))和Tim brooks([https://www.timothybrooks.com/about/](https://link.zhihu.com/?target=https%3A//www.timothybrooks.com/about/))，Bill Peebles 在伯克利人工智能研究所完成了博士学位，导师是Alyosha Efros。在此之前，他在麻省理工学院攻读本科，指导老师是Antonio Torralba。他曾在FAIR、Adobe研究院和NVIDIA实习。 Tim brooks 在伯克利人工智能研究所获得了博士学位，导师是Alyosha Efros，他是InstructPix2Pix的作者。在此之前他曾在谷歌工作，参与Pixel手机相机的研发，在NVIDIA从事视频生成模型的研究。 Sora团队的Leader是Aditya Ramesh([http://adityaramesh.com/](https://link.zhihu.com/?target=http%3A//adityaramesh.com/))， 他是DALLE、DALLE2、DALLE3的主要作者。

## **模块拆解**

### **Overview**

![img](https://heguang-tech-1300607181.cos.ap-shanghai.myqcloud.com/uPic/v2-23716e208801b29ff27940bd567bf2c1_1440w.webp)

Sora整体架构拆解

Sora的核心思想还是来自于LDM这篇文章，利用Encoder从视频中抽取视频latent，利用Decoder根据视频latent进行解码，生成最终的视频。 Diffusion Transformer模块用来根据输入的视频噪声，在给定特定条件下，进行去躁过程，最终得到视频的latent。

和之前的工作不同的地方在于Sora是完全重新训练了一个视频的Encoder来保证视频latent可以很好的保留视频的时空信息，通过Diffusion Transformer来利用scaling law把模型scale up，让diffusion model能够海量数据中学习记忆更复杂的规律。

**Visusal encoder**

本章节将分别介绍Encoder的实现、如何支持变长、不同分辨率输入。

![img](https://heguang-tech-1300607181.cos.ap-shanghai.myqcloud.com/uPic/v2-c3d4dc3949e584bf8a6ad0bcb126d9aa_1440w.webp)

from: https://openai.com/research/video-generation-models-as-world-simulators#fn-20

输入的视频可以看成是NxHxW的若干帧图像， 通过Encoder被切分成spatial tempral patch，这些patch最终会被flatten成一维向量，送入diffusion model。

这里的Encoder根据openai的资料来看可能是一个Video transformer，把输入的视频划分成若干个tuplet，每个tuplet会变成一个token，经过spatial temperal attention进行空间和时间建模获得有效的视频表征token，即上面灰色block部分。



![img](https://heguang-tech-1300607181.cos.ap-shanghai.myqcloud.com/uPic/v2-7189d456dc314e6a5be66618ed4b1124_1440w.webp)

Arnab, Anurag, et al. &quot;Vivit: A video vision transformer.&quot; Proceedings of the IEEE/CVF international conference on computer vision. 2021



常见的encoder有如下几种范式，其中第一种是时空联合建模，通过spatial-tempral attention直接建模，这种方式在大数据量情况下效果最好，但是对于小的数据集，factorised方法将时间和空间解耦建模，相当于减少模型参数，会更容易训练和效果更好。基于openai大力出奇迹的惯性，我们推测他们采用了大量的数据，采用了时空联合建模的方式，进行了video encoder的训练。



![img](https://heguang-tech-1300607181.cos.ap-shanghai.myqcloud.com/uPic/v2-264b3aafca232a5aa83768db5df7740c_1440w.webp)

Vivit: A video vision transformer



根据Openai的report，Sora支持不同长度、不同分辨率的输入支持，可以推测在训练的时候，会使用不同分辨率、不同时长的视频进行训练，从而保证推理时在不同长度和分辨率上的效果。

![img](https://heguang-tech-1300607181.cos.ap-shanghai.myqcloud.com/uPic/v2-9a75da5052da4579b7c2bd21fdf58a46_1440w.webp)

from: https://openai.com/research/video-generation-models-as-world-simulators#fn-20

但是不同的分辨率输入在训练时候带来的是大量的计算负载不均衡，一个最简单的做法就是直接padding到固定大小这种做大会引入大量不必要的计算量， 我们从openai的reference中推测他可能使用了google的NaVit里的技术降低了计算量，支持动态输入。具体展示如下：



![img](https://heguang-tech-1300607181.cos.ap-shanghai.myqcloud.com/uPic/v2-00328a589bc8ea8b1667949864f453d3_1440w.webp)

Dehghani, Mostafa, et al. &quot;Patch n&#39;Pack: NaViT, a Vision Transformer for any Aspect Ratio and Resolution.&quot; arXiv preprint arXiv:2307.06304 (2023)

![img](https://heguang-tech-1300607181.cos.ap-shanghai.myqcloud.com/uPic/v2-2248f055fca115c00f577213833ed19a_1440w.webp)

ViVit在训练性能上有显著提升的同时，能很好的保持精度

### **Diffusion Transformer**

Sora的主要作者是Peebles William，他在ICCV上发表了一篇Dit的工作，这篇工作是通过结合diffusion model和transformer，从而达到可以scale up model来提升图像生成质量的效果，这篇文章是在technical report的reference中给出，直观来讲把图像的scaling技术运用到视频场景也非常直观，可以确定是Sora的技术之一。下图也是openai用以展示训练算力scale up后视频生成质量有所提升。



![img](https://heguang-tech-1300607181.cos.ap-shanghai.myqcloud.com/uPic/v2-1b152272491f54540a12b1bab9b08e42_1440w.webp)





下图展示了Dit的主要原理，输入是一张256x256x3的图片，对图片做切patch后经过投影得到每个patch的token，得到32x32x4的latent(在推理时输入直接是32x32x4的噪声)，结合当前的step t， 输入label y作为输入， 经过N个Dit Block通过mlp进行输出，得到输出的噪声以及对应的协方差矩阵，经过T个step的预测和采样，得到32x32x4的降噪后的latent。



![img](https://heguang-tech-1300607181.cos.ap-shanghai.myqcloud.com/uPic/v2-66dc28a3e35e3c2c9a9176d2a352c3c0_1440w.webp)

Peebles, William, and Saining Xie. &quot;Scalable diffusion models with transformers.&quot; Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023



**Transformer Decoder**

“We also train a corresponding decoder model that maps generated latents back to pixel space. ”

这里猜测使用了VAE的思想，结合前面训练得到的visual encoder，对应训练了一个visual decoder，把diffsuion transformer得到的latent decoder到像素空间。

**如何保证长视频的质量**

我们都被Sora中视频的长度以及视频的一致性所震撼，那么他是如何做到的？

首先训练数据一定是下了很多功夫，从report中我们也看到openai使用了类似DALLE3的captionining技术，训练了自己的video captioner，用以给视频生成详尽的文本描述，进行模型训练。

其次为了保证视频的一致性，模型层应该不是通过多个stage方式来进行视频预测，而是整体预测了整个视频的latent，同时在训练过程中应该引入了auto regressive的task来去帮助模型更好的进行视频特征和帧间关系的学习。具体可以参考谷歌的W.A.L.T([https://arxiv.org/abs/2312.06662](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2312.06662))工作。

![img](https://heguang-tech-1300607181.cos.ap-shanghai.myqcloud.com/uPic/v2-bbf586f372c26a9974ad01631fc973ea_1440w.webp)

W.A.L.T(https://arxiv.org/abs/2312.06662)

**下游任务应用**

openAI的网站上也提到了不同任务上Sora的使用和表现，那么背后到底是怎么做的呢？ 这个章节给出详细的介绍。

1. 文生视频/文+图生视频

![img](https://heguang-tech-1300607181.cos.ap-shanghai.myqcloud.com/uPic/v2-5c632619e8331ca6b045ee4037cea342_1440w.webp)

给定初始图片，以及对应的文本： A Shiba Inu dog wearing a beret and black turtleneck.

![img](https://heguang-tech-1300607181.cos.ap-shanghai.myqcloud.com/uPic/v2-41d1c8d796810645c29cf6512b9b4b33_1440w.webp)

文生视频只需要通过文本condition的嵌入即可完成在文本条件下的视频生成，图生视频个人觉得图的条件不是通过condition的方式送给diffusion transfomer，而是通过在输入噪声的第一帧，把噪声图像替换为对应的图片经过encoder后的latent， 再送给diffusion transformer去做扩散得到最终的latent，解码成对应的视频。



![img](https://heguang-tech-1300607181.cos.ap-shanghai.myqcloud.com/uPic/v2-7881f8b19a5a819a8b6e090ee84264b0_1440w.webp)

Sora图生视频原理

2. 视频扩展

在openapi的tech report中我们还看到两类， 给定截止视频，让所有生成的视频都以此视频结尾。

![img](https://heguang-tech-1300607181.cos.ap-shanghai.myqcloud.com/uPic/v2-15b0316edd668bc3b57184bba8e31a56_1440w.webp)

![img](https://heguang-tech-1300607181.cos.ap-shanghai.myqcloud.com/uPic/v2-4f719f3486a49da50ada94d5c3cfce0d_1440w.webp)

或者给定起止视频，让生成的视频从给定的视频生成，自动生成过渡视频，最后以给定的截止视频作为结尾。

![img](https://heguang-tech-1300607181.cos.ap-shanghai.myqcloud.com/uPic/v2-1044087f100498f1f740de6df7b2416c_1440w.webp)

![img](https://heguang-tech-1300607181.cos.ap-shanghai.myqcloud.com/uPic/v2-c3706d98b9d69c9d6aed8c0e7e5ae47d_1440w.webp)

![img](https://heguang-tech-1300607181.cos.ap-shanghai.myqcloud.com/uPic/v2-103e0784ce22be015fda8f3d3fffecc0_1440w.webp)

以上两种方式的原理其实可以理解为下图，把起止视频分别使用Visual Encoder获取latent，然后加入到输入噪声中，进行扩散和解码生成即可。



![img](https://heguang-tech-1300607181.cos.ap-shanghai.myqcloud.com/uPic/v2-bd25a9fb739f16e3eb2ee502de98a752_1440w.webp)

### Computation Cost

一分钟长度、每秒30帧的视频，平均每帧包含256个token，总计将产生460ktoken，以34B模型(这里只是一个个人猜测)，需要7xA100资源推理

Dit XL 输入512x512， 训练使用一个TPU v3-256 pod， 按照TFLOPS换算约等于105个A100。 那么Sora需要的训练和微调的资源会是多少？？

![img](https://heguang-tech-1300607181.cos.ap-shanghai.myqcloud.com/uPic/v2-537e538f8cdbb40b40f8ef51bffc9727_1440w.webp)

Dit 训练实验描述



**Last but not Least**

“AGI will be able to simulate the physical world, and Sora is a key step in that direction” by Tim brooks([https://www.timothybrooks.com/tech/](https://link.zhihu.com/?target=https%3A//www.timothybrooks.com/tech/))， 这句话说明当前的Sora还在通用世界模型的方向上努力和前进，但是还并没能完全达到理解物理世界的能力。

开源社区加油，中国加油！

欢迎大家关注魔搭知乎[ModelScope小助理](https://www.zhihu.com/people/cheng-chen-16-43)， 以及ModelScope github([https://github.com/modelscope](https://link.zhihu.com/?target=https%3A//github.com/modelscope))相关动态

如果错误之处，欢迎及时指正

**Reference**

- Rombach, Robin, et al. "High-resolution image synthesis with latent diffusion models." *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*. 2022
- Peebles, William, and Saining Xie. "Scalable diffusion models with transformers." Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023
- Dehghani, Mostafa, et al. "Patch n'Pack: NaViT, a Vision Transformer for any Aspect Ratio and Resolution." *arXiv preprint arXiv:2307.06304* (2023)
- Arnab, Anurag, et al. "Vivit: A video vision transformer." *Proceedings of the IEEE/CVF international conference on computer vision*. 2021
- Gupta A, Yu L, Sohn K, et al. Photorealistic video generation with diffusion models[J]. arXiv preprint arXiv:2312.06662, 2023.
- Bao, Fan, et al. "All are worth words: a vit backbone for score-based diffusion models." *arXiv preprint arXiv:2209.12152* (2022).
- [https://openai.com/research/vid](https://link.zhihu.com/?target=https%3A//openai.com/research/video-generation-models-as-world-simulators)





:::tip 版权说明

作者：only

链接：https://zhuanlan.zhihu.com/p/682970729

:::